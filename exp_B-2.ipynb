{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"exp_B-2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1OX0TVQGZSBfR9s_-TOLakwLdhch3MwMj","authorship_tag":"ABX9TyMwNV3BVD0i4n9Tzp6JmXZQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VE3EflJsb6UC"},"source":["1. upload BraTS2021_Training_Data.tar to google drive\n","2. unzip BraTS2021_Training_Data.tar to /content/brats\n","3. install monai packages\n","4. upload the pre-trained model to /content\n","5. upload train_labels.csv to /content"]},{"cell_type":"code","metadata":{"id":"otZsDSrQJD0H"},"source":["!mkdir brats\n","!7z x -aos /content/drive/MyDrive/RSNA_seg/BraTS2021_Training_Data.tar -o/content/brats\n","!pip install monai\n","!cp /content/drive/MyDrive/exp_b_1_pretrained.h5 /content\n","!cp /content/drive/MyDrive/train_labels.csv /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCoohPncILt-","executionInfo":{"status":"ok","timestamp":1635601935295,"user_tz":-60,"elapsed":20617,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["import os\n","import cv2\n","import glob\n","import PIL\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from skimage import data\n","from skimage.util import montage \n","import skimage.transform as skTrans\n","from skimage.transform import rotate\n","from skimage.transform import resize\n","from PIL import Image, ImageOps  \n","\n","import scipy\n","import nibabel as nib\n","from monai.transforms import Compose, Resize,AddChannel\n","import monai\n","\n","import keras\n","import keras.backend as K\n","from keras.callbacks import CSVLogger\n","import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from tensorflow.keras.models import *\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n","from tensorflow.keras.layers.experimental import preprocessing\n","from keras.layers import *\n","from keras.models import *\n","\n","np.set_printoptions(precision=3, suppress=True)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PrxB65YJ2Mr","executionInfo":{"status":"ok","timestamp":1635601936103,"user_tz":-60,"elapsed":2,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["# DEFINE seg-areas  \n","SEGMENT_CLASSES = {\n","    0 : 'NOT tumor',\n","    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n","    2 : 'EDEMA',\n","    3 : 'ENHANCING' # original 4 -> converted into 3 later\n","}\n","\n","# there are 155 slices per volume\n","# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \n","VOLUME_SLICES = 100 \n","VOLUME_START_AT = 22 # first slice of volume that we will include\n","IMG_SIZE1=128\n","IMG_SIZE2=128\n","IMG_SIZE3=80\n","\n","TRAIN_DATASET_PATH = '/content/brats'\n","\n","pre_trained_model_path = \"/content/exp_b_1_pretrained.h5\"\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wiYj3EcMcj7Y"},"source":["In encoder part, i set their trainable=False, except lats layer"]},{"cell_type":"code","metadata":{"id":"weHXKqGoQ5F_","executionInfo":{"status":"ok","timestamp":1635601948736,"user_tz":-60,"elapsed":12635,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["def build_unet(inputs):\n","    conv11 = Conv3D(32, (3, 3, 3), activation='relu', padding='same', trainable=False)(inputs)\n","    conc11 = concatenate([inputs, conv11], axis=4)\n","    conv12 = Conv3D(32, (3, 3, 3), activation='relu', padding='same', trainable=False)(conc11)\n","    conc12 = concatenate([inputs, conv12], axis=4)\n","    pool1 = MaxPooling3D(pool_size=(2, 2, 2), trainable=False)(conc12)\n","\n","    conv21 = Conv3D(64, (3, 3, 3), activation='relu', padding='same', trainable=False)(pool1)\n","    conc21 = concatenate([pool1, conv21], axis=4)\n","    conv22 = Conv3D(64, (3, 3, 3), activation='relu', padding='same', trainable=False)(conc21)\n","    conc22 = concatenate([pool1, conv22], axis=4)\n","    pool2 = MaxPooling3D(pool_size=(2, 2, 2), trainable=False)(conc22)\n","\n","    conv31 = Conv3D(128, (3, 3, 3), activation='relu', padding='same', trainable=False)(pool2)\n","    conc31 = concatenate([pool2, conv31], axis=4)\n","    conv32 = Conv3D(128, (3, 3, 3), activation='relu', padding='same', trainable=False,name='123')(conc31)\n","    conc32 = concatenate([pool2, conv32], axis=4)\n","    pool3 = MaxPooling3D(pool_size=(2, 2, 2),name='pool3', trainable=False)(conc32)\n","\n","    conv41 = Conv3D(256, (3, 3, 3), activation='relu', padding='same',name='conv41', trainable=False)(pool3)\n","    conc41 = concatenate([pool3, conv41], axis=4)\n","    conv42 = Conv3D(256, (3, 3, 3), activation='relu', padding='same', trainable=False)(conc41)\n","    conc42 = concatenate([pool3, conv42], axis=4)\n","    pool4 = MaxPooling3D(pool_size=(2, 2, 2), trainable=False)(conc42)\n","\n","    conv51 = Conv3D(512, (3, 3, 3), activation='relu', padding='same',name='mid_feature_1')(pool4)\n","    conc51 = concatenate([pool4, conv51], axis=4,name='mid_feature')\n","\n","    conv52 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(conc51)\n","    conc52 = concatenate([pool4, conv52], axis=4,name='mid_feature_2')\n","\n","    up6 = concatenate([Conv3DTranspose(256, (2, 2, 2), strides=(2, 2, 2), padding='same')(conc52), conc42], axis=4)\n","    conv61 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(up6)\n","    conc61 = concatenate([up6, conv61], axis=4)\n","    conv62 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conc61)\n","    conc62 = concatenate([up6, conv62], axis=4)\n","\n","    up7 = concatenate([Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same')(conc62), conv32], axis=4)\n","    conv71 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(up7)\n","    conc71 = concatenate([up7, conv71], axis=4)\n","    conv72 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conc71)\n","    conc72 = concatenate([up7, conv72], axis=4)\n","\n","    up8 = concatenate([Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same')(conc72), conv22], axis=4)\n","    conv81 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(up8)\n","    conc81 = concatenate([up8, conv81], axis=4)\n","    conv82 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conc81)\n","    conc82 = concatenate([up8, conv82], axis=4)\n","\n","    up9 = concatenate([Conv3DTranspose(32, (2, 2, 2), strides=(2, 2, 2), padding='same')(conc82), conv12], axis=4)\n","    conv91 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(up9)\n","    conc91 = concatenate([up9, conv91], axis=4)\n","    conv92 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conc91)\n","    conc92 = concatenate([up9, conv92], axis=4)\n","\n","    conv10 = Conv3D(4, (1, 1, 1), activation='softmax',name='conv10')(conc92)\n","\n","    return Model(inputs=inputs, outputs=conv10)\n","\n","input_layer = Input((IMG_SIZE1, IMG_SIZE2,IMG_SIZE3, 1))\n","\n","model = build_unet(input_layer)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fE4-OJQec0-H"},"source":["load pre-trained model"]},{"cell_type":"code","metadata":{"id":"BYb_pl20lv9J","executionInfo":{"status":"ok","timestamp":1635601949380,"user_tz":-60,"elapsed":650,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["model.load_weights(pre_trained_model_path)  "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"kfeuvUbqlunj","executionInfo":{"status":"ok","timestamp":1635601949381,"user_tz":-60,"elapsed":2,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["new_model = Model(inputs=model.input,outputs=model.get_layer('mid_feature').output)\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8C4yDviZdIQ1"},"source":["connect encoder and classification model"]},{"cell_type":"code","metadata":{"id":"bAkzssyMmwEE","executionInfo":{"status":"ok","timestamp":1635601984081,"user_tz":-60,"elapsed":328,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["def final_model(g):\n","    model_x = Sequential()\n","    model_x.add(g)\n","    model_x.add(Conv3D(256,(3,3,3)))\n","    model_x.add(Activation('relu'))\n","    model_x.add(MaxPooling3D(pool_size=(3, 3, 3),strides=(1,1,1)))\n","    model_x.add(Flatten())\n","    model_x.add(Dense(1024))\n","    model_x.add(Activation('relu'))\n","    model_x.add(Dense(1))\n","    model_x.add(Activation('sigmoid'))\n","    return model_x"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7JFMFyxn6mY","executionInfo":{"status":"ok","timestamp":1635601985061,"user_tz":-60,"elapsed":2,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["another_model = final_model(new_model)\n","another_model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.000002),metrics = ['accuracy',tf.keras.metrics.AUC()])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmXJC1RFLDfQ","executionInfo":{"status":"ok","timestamp":1635601985948,"user_tz":-60,"elapsed":2,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["cls_df = pd.read_csv('train_labels.csv')\n","cls_id = cls_df['BraTS21ID'].values\n","cls_label = cls_df['MGMT_value'].values\n","\n","d1=zip(cls_id,cls_label)\n","label_dict = dict(d1)\n","\n","# lists of directories with studies\n","train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n","\n","def pathListIntoIds(dirList):\n","    x = []\n","    for i in range(0,len(dirList)):\n","        x.append(dirList[i][dirList[i].rfind('/')+1:])\n","    return x\n","\n","train_and_test_ids = pathListIntoIds(train_and_val_directories); \n","\n","only_id = []\n","new_train_and_test_ids = [] # this is our hybrid dataset containing mri, masks, labels\n","for i in range(len(train_and_test_ids)):\n","  only = int(train_and_test_ids[i].split('_')[-1])\n","  if only in cls_id:\n","    only_id.append(only)\n","    new_train_and_test_ids.append(train_and_test_ids[i])\n","\n","train_ids, val_ids = train_test_split(new_train_and_test_ids,test_size=0.2,random_state=42) "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xV8EBBthdmZB"},"source":["return 2 variables: X: MRI images, label_z: label of methylation"]},{"cell_type":"code","metadata":{"id":"DV4SjSPrOOgf","executionInfo":{"status":"ok","timestamp":1635601987793,"user_tz":-60,"elapsed":288,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["class DataGenerator(tf.keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, dim=(IMG_SIZE1,IMG_SIZE2,IMG_SIZE3), batch_size = 1, n_channels = 1, shuffle=True, is_train = True, label = None):\n","        'Initialization'\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.list_IDs = list_IDs\n","        self.n_channels = n_channels\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","        self.transforms = Compose([AddChannel(),Resize((IMG_SIZE1,IMG_SIZE2,IMG_SIZE3))])\n","        self.is_train = is_train\n","        self.label = label\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        Batch_ids = [self.list_IDs[k] for k in indexes]\n","\n","\n","        # Generate data\n","        X, y = self.__data_generation(Batch_ids)\n","\n","        label_z = np.zeros(len(Batch_ids))\n","        for x in range(len(Batch_ids)):\n","          x1 = int(Batch_ids[x].split('_')[-1])\n","          label_z[x]=(self.label[x1])\n","        \n","\n","\n","        if self.is_train == True:\n","          return X, label_z\n","        else:\n","          return X,y,Batch_ids\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, Batch_ids):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = np.zeros((self.batch_size, *self.dim, self.n_channels))\n","        y = np.zeros((self.batch_size, *self.dim))\n","        Y = np.zeros((self.batch_size, *self.dim, 4))\n","\n","        # Generate data\n","        for c, i in enumerate(Batch_ids):\n","            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n","\n","            data_path = os.path.join(case_path, f'{i}_flair.nii.gz');\n","            flair = nib.load(data_path).get_fdata()\n","            flair = self.transforms(flair) \n","            \n","            data_path = os.path.join(case_path, f'{i}_seg.nii.gz');\n","            seg = nib.load(data_path).get_fdata()\n","            seg = self.transforms(seg)\n","\n","            flair = flair.squeeze(0)\n","            seg = seg.squeeze(0)\n","\n","            X[c,:,:,:,0] = flair\n","            y[c] = seg\n","\n","        y[y==4] = 3\n","        mask = tf.one_hot(y, 4)\n","        return X/np.max(X), mask"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipcTuFUZqESr","executionInfo":{"status":"ok","timestamp":1635601991371,"user_tz":-60,"elapsed":312,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["training_generator = DataGenerator(train_ids,label=label_dict)\n","valid_generator = DataGenerator(val_ids,shuffle=False,label=label_dict)\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1OrJspFtEuI","executionInfo":{"status":"ok","timestamp":1635601992297,"user_tz":-60,"elapsed":1,"user":{"displayName":"max bay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14342188811040772920"}}},"source":["callbacks = [\n","      # keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.000001, verbose=1),\n","      CSVLogger('training.log', separator=',', append=False),\n"," keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.h5',verbose=1, save_best_only=True, save_weights_only = True)\n","        # csv_logger\n","    ]"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyENbfCRtIIf"},"source":["K.clear_session()\n","\n","history =  another_model.fit(training_generator,\n","            epochs=35,\n","            steps_per_epoch=len(train_ids),\n","            callbacks= callbacks,\n","            validation_data = valid_generator\n","                    )  "],"execution_count":null,"outputs":[]}]}