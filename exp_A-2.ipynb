{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"exp_A-2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1FPriMFa0XKGdxS9rdVbyO1a1-ewY80kr","authorship_tag":"ABX9TyMcG496hFRGeNSaEQkeLo8Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"B-BM5eSPLXSf"},"source":["1. upload png_data.zip to google drive\n","2. unzip png_data.zip to /content\n","3. install monai, torchio, pydicom packages\n","\n"]},{"cell_type":"code","metadata":{"id":"Bke8jQPNM0kZ"},"source":["!7z x -aos /content/drive/MyDrive/png_data.zip -o/content\n","!pip install pydicom\n","!pip install torchio\n","!pip install monai"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y00-ZGpENQor"},"source":["import glob\n","import os\n","import cv2\n","import torch\n","from torch.utils.data import Dataset,DataLoader\n","import pydicom\n","import numpy as np\n","from pydicom.pixel_data_handlers.util import apply_voi_lut\n","from torchvision import transforms, utils\n","import pandas as pd\n","from PIL import Image\n","from sklearn.metrics import roc_auc_score\n","\n","import torchio as tio\n","from sklearn.model_selection import train_test_split\n","import re\n","import matplotlib.pyplot as plt\n","import monai\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1XPaQRYPR3ZG"},"source":["In experiment A-2, I  train densenet-121 and efficientnet-b0  model.\n","\n","So  available model_selected = ['densenet', 'effcientnet']\n","\n","change the model_selected to do experiment A-2"]},{"cell_type":"code","metadata":{"id":"PdU9L7a4Pv4g"},"source":["data_directory='/content/png_voxel_converted_ds'\n","NUM_IMAGES=36\n","IMAGE_SIZE=224\n","\n","model_selected = 'densenet' # densenet or efficientnet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NPSMrQaNSmd"},"source":["def load_2dimage(file,rotate= 0):\n","  img = Image.open(file)\n","  img = np.array(img)\n","\n","\n","  height, width = img.shape[:2]\n","  center = (width // 2, height // 2)\n","      \n","  if rotate != 0:\n","    M = cv2.getRotationMatrix2D(center, rotate, 1)\n","    img = cv2.warpAffine(img, M, (width, height))\n","    \n","  img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n","\n","  return img\n","\n","\n","def load_3dimage(ids, num_imgs=NUM_IMAGES, split='train', img_size=IMAGE_SIZE, mri_type='FLAIR',rotate = 0):\n","  files = sorted(glob.glob(f\"{data_directory}/{split}/{ids}/{mri_type}/*.png\"), \n","               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n","  middle = len(files)//2\n","  num_imgs2 = num_imgs//2\n","  p1 = max(0, middle - num_imgs2)\n","  p2 = min(len(files), middle + num_imgs2)\n","  img3d = np.stack([load_2dimage(f,rotate = rotate) for f in files[p1:p2]]).T \n","\n","  if img3d.shape[-1] < num_imgs:\n","    n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n","    img3d = np.concatenate((img3d,  n_zero), axis = -1)\n","        \n","  if np.min(img3d) < np.max(img3d):\n","    img3d = img3d - np.min(img3d)\n","    img3d = img3d / np.max(img3d)\n","  return img3d\n","\n","\n","class PNG_dataset(Dataset):\n","  def __init__(self, df, is_train=True,test = False, split='train', mritype='FLAIR',label_smoothing = 0.01):\n","    self.ids = df[\"BraTS21ID\"].values\n","    self.y =  df[\"MGMT_value\"].values\n","    self.is_train = is_train\n","    self.mritype = mritype\n","    self.label_smoothing = label_smoothing\n","    self.test = test\n","    self.split = split\n","\n","  def __len__(self):\n","    return len(self.ids)\n","\n","  def __getitem__(self, idx):\n","    id1 = str(self.ids[idx]).zfill(5)\n","    label = self.y[idx]\n","    ro = 0\n","    \n","    # concatenate all mri images by their depth\n","    list_x_flair =  load_3dimage(id1, split = self.split, mri_type = 'FLAIR', rotate = ro)\n","    list_x_t1w =  load_3dimage(id1, split = self.split, mri_type = 'T1w', rotate = ro)\n","    list_x_t1wce =  load_3dimage(id1, split = self.split, mri_type = 'T1wCE', rotate = ro)\n","    list_x_t2w =  load_3dimage(id1, split = self.split, mri_type = 'T2w', rotate = ro)\n","    list_x = np.concatenate((list_x_flair,list_x_t1w,list_x_t1wce,list_x_t2w),axis=2)\n","\n","    opti_trans_1 = {\n","          tio.RandomMotion():0.3,\n","          tio.RandomBiasField():0.3,\n","          }\n","    opti_trans_2 = {\n","        tio.RandomFlip():0.5,\n","        tio.RandomAnisotropy():0.5,\n","    }\n","    transforms_io = tio.Compose([\n","          tio.OneOf(opti_trans_1, p=0.4),\n","          tio.OneOf(opti_trans_2, p=0.4),\n","          tio.RandomNoise(p=0.15),\n","          tio.RescaleIntensity(out_min_max=(-1, 1))\n","          ])\n","    \n","    transforms_io_test = tio.Compose([\n","          tio.RescaleIntensity(out_min_max=(-1, 1))\n","          ])\n","\n","    transform = transforms.Compose([\n","                transforms.ToTensor()\n","    ])\n","    # \n","    list_x = transform(list_x) \n","    list_x = list_x.unsqueeze(0)\n","\n","\n","    if self.is_train == True:\n","      list_x = transforms_io(list_x)\n","    else:\n","      list_x = transforms_io_test(list_x)\n","\n","    \n","    if self.test == True:\n","      return torch.as_tensor(list_x, dtype=torch.float), id1\n","    else:\n","      return torch.as_tensor(list_x, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiNxA9RiVL70"},"source":["origin_df = pd.read_csv('/content/train_labels.csv')\n","df_train, df_valid1 = train_test_split(origin_df, test_size=0.2, random_state=42,stratify=origin_df[\"MGMT_value\"])\n","df_valid, df_test = train_test_split(df_valid1, test_size=0.5, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LopIRsppVm2U"},"source":["train_dataset = PNG_dataset(df_train, is_train=True)\n","val_dataset = PNG_dataset(df_valid,is_train=False)\n","test_dataset = PNG_dataset(df_test,is_train=False)\n","\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n","\n","print(\"len of train data\", len(train_dataset))\n","print(\"len of valid data\", len(val_dataset))\n","print(\"len of train batch\", len(train_loader))\n","print(\"len of valid batch\", len(val_loader))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igXQJmN1ZUa9"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if model_selected == 'efficientnet':\n","  model = monai.networks.nets.EfficientNetBN(\"efficientnet-b0\", spatial_dims=3, in_channels=1, num_classes=2).to(device)\n","if model_selected == 'densenet':\n","  model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)\n","\n","loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n","\n","# scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.9**epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrZHWz_FL9OI"},"source":["val_interval = 1 \n","best_metric = -1\n","best_auc = -1\n","best_metric_epoch = -1\n","best_auc_epoch = -1\n","epoch_loss_values = list()\n","metric_values = list()\n","writer = SummaryWriter()\n","\n","train_loss = []\n","valid_loss = []\n","test_loss = []\n","valid_acc = []\n","valid_auc= []\n","test_acc = []\n","test_auc = []\n","for epoch in range(200):\n","  print(\"-\" * 10)\n","  print(f\"epoch {epoch + 1}/{200}\")\n","  model.train()\n","  epoch_loss = 0\n","  step = 0\n","  val_step = 0\n","  for batch_data in train_loader:\n","    step += 1\n","    inputs, labels = batch_data\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = loss_function(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    epoch_loss += loss.item()\n","\n","  current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n","  writer.add_scalar(\"learning rate\", current_lr, epoch+1)\n","  # scheduler.step()\n","  epoch_loss /= step\n","  epoch_loss_values.append(epoch_loss) # avg train loss\n","  train_loss.append(epoch_loss)\n","  writer.add_scalar(\"train_loss\", epoch_loss, epoch+1)\n","\n","  print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n","\n","  if (epoch + 1) % val_interval == 0:\n","    model.eval()\n","    with torch.no_grad():\n","      num_correct = 0.0\n","      metric_count = 0\n","      all_labels = []\n","      prob = []\n","      val_step=0\n","      val_loss_epoch=0\n","      for val_data in val_loader:\n","        val_step += 1\n","        val_images, val_labels = val_data\n","        val_images = val_images.to(device)\n","        val_labels = val_labels.to(device)\n","        val_outputs = model(val_images)\n","        val_loss = loss_function(val_outputs, val_labels)\n","        value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n","        val_loss_epoch += val_loss.item()\n","        metric_count += len(value)\n","        num_correct += value.sum().item()\n","        prob_positive = torch.softmax(val_outputs, dim=1)[:, 1]\n","        prob_positive111 = prob_positive.cpu().numpy()\n","        for a in prob_positive111:\n","          prob.append(a)\n","        x_labels = val_labels.cpu().numpy()\n","        for a in x_labels:\n","          all_labels.append(a)\n","\n","      val_auc = roc_auc_score(all_labels,prob)\n","      avg_val_loss = val_loss_epoch / val_step\n","      valid_loss.append(avg_val_loss) # avg valid loss\n","      valid_auc.append(val_auc) #avg valid auc\n","      \n","      metric = num_correct / metric_count\n","      valid_acc.append(metric)\n","      writer.add_scalar(\"valid_loss\", avg_val_loss, epoch+1)\n","      writer.add_scalar(\"val_accuracy\", metric, epoch + 1)\n","      writer.add_scalar(\"val_AUC\", val_auc, epoch + 1)\n","\n","  # below is test part\n","  if (epoch + 1) % val_interval == 0:\n","    model.eval()\n","    with torch.no_grad():\n","      num_correct = 0.0\n","      metric_count = 0\n","      all_labels = []\n","      prob = []\n","      val_step=0\n","      val_loss_epoch=0\n","      for val_data in test_loader:\n","        val_step += 1\n","        val_images, val_labels = val_data\n","        val_images = val_images.to(device)\n","        val_labels = val_labels.to(device)\n","        val_outputs = model(val_images)\n","        val_loss = loss_function(val_outputs, val_labels)\n","        value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n","        val_loss_epoch += val_loss.item()\n","        metric_count += len(value)\n","        num_correct += value.sum().item()\n","\n","        prob_positive = torch.softmax(val_outputs, dim=1)[:, 1]\n","        prob_positive111 = prob_positive.cpu().numpy()\n","        for a in prob_positive111:\n","          prob.append(a)\n","        x_labels = val_labels.cpu().numpy()\n","        for a in x_labels:\n","          all_labels.append(a)\n","\n","      auc1 = roc_auc_score(all_labels,prob)\n","      avg_val_loss = val_loss_epoch / val_step\n","      test_loss.append(avg_val_loss)\n","      test_auc.append(auc1)\n","      \n","      metric = num_correct / metric_count\n","      test_acc.append(metric)\n","      writer.add_scalar(\"test_loss\", avg_val_loss, epoch+1)\n","      writer.add_scalar(\"test_accuracy\", metric, epoch + 1)\n","      writer.add_scalar(\"test_AUC\", auc1, epoch + 1)\n","\n","  torch.save(model.state_dict(), f\"epoch{epoch+1}.pth\")\n","\n","print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n","writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3ctYqxwkRz4"},"source":["record1 = pd.DataFrame()\n","record1['train_loss'] = train_loss\n","record1['valid_loss'] = valid_loss\n","record1['test_loss'] = test_loss\n","record1['valid_acc'] = valid_acc\n","record1['valid_auc'] = valid_auc\n","record1['test_acc'] = test_acc\n","record1['test_auc'] = test_auc\n","record1.to_csv('experiment_2_dense_tmp2.csv')"],"execution_count":null,"outputs":[]}]}